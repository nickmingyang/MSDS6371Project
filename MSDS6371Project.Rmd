---
title: "MSDS6371Project"
author: "Allen Miller & Mingyang Nick YU"
date: "11/19/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### Read in Training Data and initial inspection:
```{r}
#Load required libraries
library(tidyverse)
library(ggplot2)
library(olsrr)
library(GGally)
library(caret)
library(MASS)
library(leaps)
#Load in Dataset
#Call original data amesHouse
amesHouse = read.csv("/Users/mingyang/Desktop/SMU/StatisticalFoundation_Fall2020/MSDS6371Project/train.csv",header = TRUE)
#observe data
head(amesHouse)
str(amesHouse)
#look at neighborhood
amesHouse$Neighborhood
```

#### Analysis 1:
```{r}
#extract variables we're interested for Analysis 1
data1 = amesHouse %>% dplyr::select(SalePrice,GrLivArea,Neighborhood)%>%
  filter(Neighborhood=="NAmes"|Neighborhood=="Edwards"|Neighborhood=="BrkSide")
str(data1)
#383 observations selected after filtering neighborhood of "NAmes", "Edwards" and "BrkSide"
#Convert Neighborhood to factors
data1$Neighborhood = as.factor(data1$Neighborhood)
#Plot and observe the relationship between SalePrice and GrLivArea
data1 %>% ggplot(aes(x=GrLivArea,y=SalePrice))+
  geom_point()+ggtitle("Sale Price vs. Square Foot of Living area")+
  xlab("Square Foot of Living Area")+
  ylab("Sale Price")
#there are some outliers that may not have come from the same population of interest
#build model to identify the outliers
model1 = lm(SalePrice~GrLivArea,data=data1)
summary(model1)
#Look at residual plots and cook's distance
plot(model1)
cooksDistance = data.frame(distance = cooks.distance(model1))
cooksDistance%>%filter(distance>=1)
ols_plot_cooksd_bar(model1)
ols_plot_resid_stand(model1)
#ols_plot_resid_lev(model1)
# Observation 339 has cook's distance larger than 5.6, Observation 131's cook's D is larger than 1 so it may due to the face it is a unique case.
# Observation 169 and 190 has standarlized residual greater than 4, these two observations can also potentially not coming from the same population of interest
# Since our sample is sufficiently large, deleting these four outliers won't make a huge difference.
# Call these new dataset without outliers data2
data2 = data1[-c(131,169,190,339),]
str(data2)
#build new model to double check
model2 = lm(SalePrice~GrLivArea,data=data2)
summary(model2)
plot(model2)
ols_plot_cooksd_bar(model2)
ols_plot_resid_stand(model2)
#Cook's Distance within roughly 0.1, residual within 3, decent amount of observations beyond 2 (within 5%)
#Assumptions met, move on to include categorical variables
model3 = lm(SalePrice~GrLivArea+Neighborhood+GrLivArea*Neighborhood,data=data2)
summary(model3)
plot(model3)
ols_plot_cooksd_bar(model3)
ols_plot_resid_stand(model3)
#Assumptions are met, model 3 has higher Adjusted R-squared compared to model2

```

#### Analysis 2 - Variable converting & Inspecting relationship:
```{r}
# convert certain columns we need into factor in order to use multi regression model
cols.to.factor = c("MSSubClass","MSZoning","Street","Alley","LotShape","LandContour","Utilities",
                   "LotConfig","LandSlope","Neighborhood","Condition1","Condition2","BldgType",
                   "HouseStyle","OverallQual","OverallCond","RoofStyle","RoofMatl","Exterior1st",
                   "Exterior2nd","MasVnrType","ExterQual","ExterCond","Foundation","BsmtQual",
                   "BsmtCond","BsmtExposure","BsmtFinType1","BsmtFinType2","Heating","HeatingQC",
                   "CentralAir","Electrical","KitchenQual","Functional","FireplaceQu","GarageType",
                   "GarageFinish","GarageQual","GarageCond","PavedDrive","PoolQC","Fence","MiscFeature",
                   "SaleType","SaleCondition","PoolArea","MoSold","YrSold","Fireplaces","FullBath")
amesHouse[cols.to.factor] = lapply(amesHouse[cols.to.factor],factor)
str(amesHouse)

#Now exploring relationships between continuous variables
amesHouse %>% dplyr::select(SalePrice,LotFrontage,LotArea,YearBuilt,YearRemodAdd,MasVnrArea,BsmtFinSF1,BsmtFinSF2)%>%
  ggpairs()
#Try Logrithmic on SalePrice
amesHouse$lSalePrice = log(amesHouse$SalePrice)
#explore relationship between log SalePrice
amesHouse %>% dplyr::select(lSalePrice,LotFrontage,LotArea,YearBuilt,YearRemodAdd,MasVnrArea,BsmtFinSF1,BsmtFinSF2)%>%
  ggpairs()
#There seem to have a slight improvement between YearBuilt and YearRemodAdd

#Move forward with next batch of exploration
amesHouse %>% dplyr::select(SalePrice,BsmtUnfSF,TotalBsmtSF,X1stFlrSF,X2ndFlrSF,LowQualFinSF,GrLivArea) %>%
  ggpairs()
amesHouse %>% dplyr::select(lSalePrice,BsmtUnfSF,TotalBsmtSF,X1stFlrSF,X2ndFlrSF,LowQualFinSF,GrLivArea) %>%
  ggpairs()
#There seem to have a good linear relationship between lSalePrice and TotalBsmtSF, X1stFlrSF, X2ndFlrSF,GrLivArea
#correlation looks better with lSalePrice comparing to SalePrice

#Move forward with next batch of exploration
amesHouse %>% dplyr::select(SalePrice,BsmtFullBath,BsmtHalfBath,HalfBath,BedroomAbvGr,KitchenAbvGr,
                     TotRmsAbvGrd)%>%
  ggpairs()
amesHouse %>% dplyr::select(lSalePrice,BsmtFullBath,BsmtHalfBath,HalfBath,BedroomAbvGr,KitchenAbvGr,
                     TotRmsAbvGrd)%>%
  ggpairs()

#The only variable seem to have good correlation with lSalePrice is FullBath,TotRmsAbvGrd.
#???If we should change BsmtFullBath,BsmtHalfBath,HalfBath,BedroomAbvGr,KitchenAbvGr to factors???
#Keep going
amesHouse %>% dplyr::select(SalePrice,GarageYrBlt,GarageCars,GarageArea) %>% ggpairs()
amesHouse %>% dplyr::select(lSalePrice,GarageYrBlt,GarageCars,GarageArea) %>% ggpairs()
#The above variables all seem to have a good linear correlation with lSalePrice
#Keep going
amesHouse %>% dplyr::select(SalePrice,WoodDeckSF,OpenPorchSF,EnclosedPorch,X3SsnPorch,ScreenPorch,MiscVal)%>%
  ggpairs()
amesHouse %>% dplyr::select(lSalePrice,WoodDeckSF,OpenPorchSF,EnclosedPorch,X3SsnPorch,ScreenPorch,MiscVal)%>%
  ggpairs()
#linear relationships don't seem particularly strong except WoodDeckSF, OpenPorchSF
#Overall, lSalePrice seem to improve the correlation with our important predictors a bit. So we will proceed
#with lSalePrice Analysis

```
### Select key variables that can be used as predictor - based on results above
```{r}
summary(amesHouse)
#LotFrontage has too many missing values, yet it only has 0.356 correlation with lSalePrice, so not using this variable
#Alley has too many missing values, so delete column Alley
#LowQualFinSF has close to no correlaion with lSalePrice, delete
#BsmtHalfBath has close to 0 correlation,delete
#FireplaceQu too much missing data, delete
#X3SsnPorch too little correlation, delete
#PoolQC too much missing data, delete
#MiscFeature too much missing data, delete
#MiscVal too little correlation, delete
#Consder adding: Exterior1nd, Exterior2nd
#Selected variables make sense to predict lSalePrice
amesHouse$PoolYN = ifelse(is.na(amesHouse$PoolQC),"NO","YES")
amesHouse$PoolYN = as.factor(amesHouse$PoolYN)
variables.used = amesHouse %>% 
  dplyr::select(Id,MSSubClass,MSZoning,LotArea,LotConfig,Neighborhood,HouseStyle,
         OverallQual,OverallCond,YearBuilt,YearRemodAdd,MasVnrArea,ExterQual,
         ExterCond,Foundation,BsmtQual,BsmtCond,TotalBsmtSF,
         Heating,CentralAir,X1stFlrSF,GrLivArea,FullBath,KitchenQual,TotRmsAbvGrd,
         Fireplaces,GarageType,GarageCars,PoolYN,MoSold,YrSold,lSalePrice)
str(variables.used)
summary(variables.used)
#Adjusting MasVnrArea
variables.used$MasVnrArea = ifelse(is.na(variables.used$MasVnrArea),0,variables.used$MasVnrArea)
variables.used$BsmtQual = as.character(variables.used$BsmtQual)
variables.used$BsmtQual = ifelse(is.na(variables.used$BsmtQual),"NONE",variables.used$BsmtQual)
variables.used$BsmtQual = as.factor(variables.used$BsmtQual)
#Adjusting BsmtCond
variables.used$BsmtCond = as.character(variables.used$BsmtCond)
variables.used$BsmtCond = ifelse(is.na(variables.used$BsmtCond),"NONE",variables.used$BsmtCond)
variables.used$BsmtCond = as.factor(variables.used$BsmtCond)
#Adjusting GarageType
variables.used$GarageType = as.character(variables.used$GarageType)
variables.used$GarageType = ifelse(is.na(variables.used$GarageType),"NONE",variables.used$GarageType)
variables.used$GarageType = as.factor(variables.used$GarageType)

```


### Stepwise Selection
```{r}
set.seed(11)
train.control <- trainControl(method = "cv", number = 10)
step.model = train(lSalePrice~.-Id, data=variables.used,
                   method="lmStepAIC",
                   trControl = train.control,
                   trace=FALSE)
step.model$results
step.model$finalModel
summary(step.model$finalModel)
print(step.model)
# RMSE = 0.1494448 by 10 fold internal cross-validation
exp(0.1494448)
ols_plot_resid_stand(step.model$finalModel)
ols_plot_cooksd_bar(step.model$finalModel)
```

### Load in Test Data 
```{r}
#Load in test.csv
testData = read.csv("/Users/mingyang/Desktop/SMU/StatisticalFoundation_Fall2020/MSDS6371Project/test.csv",header = TRUE)
#Convert variables we need from N/A to NONE
testData$MasVnrArea = ifelse(is.na(testData$MasVnrArea),0,testData$MasVnrArea)
testData$TotalBsmtSF = ifelse(is.na(testData$TotalBsmtSF),0,testData$TotalBsmtSF)
testData$GarageCars = ifelse(is.na(testData$GarageCars),0,testData$GarageCars)
testData$MSZoning = ifelse(is.na(testData$MSZoning),"RL",testData$MSZoning)
testData$KitchenQual = ifelse(is.na(testData$KitchenQual),"TA",testData$KitchenQual)
testData$MSSubClass = ifelse(testData$MSSubClass==150,180,testData$MSSubClass)
testData$FullBath = ifelse(testData$FullBath==4,3,testData$FullBath)
testData$Fireplaces = ifelse(testData$Fireplaces==4,3,testData$Fireplaces)

testData$BsmtQual = ifelse(is.na(testData$BsmtQual),"NONE",testData$BsmtQual)
testData$BsmtCond = ifelse(is.na(testData$BsmtCond),"NONE",testData$BsmtCond)
testData$GarageType = ifelse(is.na(testData$GarageType),"NONE",testData$GarageType)
#Convert some variables we need into factor
testData[cols.to.factor] = lapply(testData[cols.to.factor],factor)
#create variable PoolYN
testData$PoolYN = ifelse(is.na(testData$PoolQC),"NO","YES")
testData$PoolYN = as.factor(testData$PoolYN)
#select variables needed for testing
testVariable = testData %>% 
  dplyr::select(Id,MSSubClass,MSZoning,LotArea,LotConfig,Neighborhood,HouseStyle,
         OverallQual,OverallCond,YearBuilt,YearRemodAdd,MasVnrArea,ExterQual,
         ExterCond,Foundation,BsmtQual,BsmtCond,TotalBsmtSF,
         Heating,CentralAir,X1stFlrSF,GrLivArea,FullBath,KitchenQual,TotRmsAbvGrd,
         Fireplaces,GarageType,GarageCars,PoolYN,MoSold,YrSold)

```

### run stepwise test and export result to csv file - Kaggle Score 0.15372
```{r}
testVariable$lSalePrice = predict(step.model,testVariable)
testVariable$SalePrice = exp(testVariable$lSalePrice)
result.stepwise = testVariable %>% dplyr::select(Id,SalePrice)
write.csv(result.stepwise,"stepwise_Prediction_Miller_YU.csv",row.names = FALSE)
```

### Forward Selection - Kaggle Score 0.15432
```{r}
set.seed(115)
#create forward fit model

#write a program to pretict 5 different combination to test forward selection model
iterations = 5
splitPerc = 0.9
total_RMSE = 0
for(i in 1:iterations){
  print(i)
  trainIndices = sample(1:dim(variables.used)[1],round(splitPerc * dim(variables.used)[1]))
  train = variables.used[trainIndices,]
  test = variables.used[-trainIndices,]
  forward_fit = lm(lSalePrice~.-Id,data=train)
  forward.model = ols_step_forward_aic(forward_fit,penter=0.15)
  prediction = predict(forward.model$model,test)
  squared_MSPE = mean((test$lSalePrice - prediction)^2)
  temp_RMSE = sqrt(squared_MSPE)
  total_RMSE = total_RMSE+temp_RMSE
}
#total RMSE = 0.1235978... RMSE = 839.29 under original SalePrice scale
total_RMSE/iterations

#Build model with full dataset for fitting
forward_fit = lm(lSalePrice~.-Id,data=variables.used)
forward.model = ols_step_forward_aic(forward_fit,penter=0.15)
forward.model$model
forward.model
ols_plot_resid_stand(forward.model$model)
ols_plot_cooksd_bar(forward.model$model)
plot(forward.model$model)

testVariable$lSalePriceForward = predict(forward.model$model,testVariable)
testVariable$SalePrice = exp(testVariable$lSalePriceForward)
result.forward = testVariable %>% dplyr::select(Id,SalePrice)
write.csv(result.forward,"Forward_Prediction_Miller_YU_2.csv",row.names = FALSE)

#Try Mass Library Forward model
forward.model2 = stepAIC(forward_fit,direction="forward")
forward.model2$model

```

### Backward Selection - Kaggle Score 0.15432
```{r}
set.seed(112)
#create forward fit model

#write a program to pretict 5 different combination to test forward selection model
iterations = 5
splitPerc = 0.95
total_RMSE = 0
for(i in 1:iterations){
  print(i)
  trainIndices = sample(1:dim(variables.used)[1],round(splitPerc * dim(variables.used)[1]))
  train = variables.used[trainIndices,]
  test = variables.used[-trainIndices,]
  backward_fit = lm(lSalePrice~.-Id,data=train)
  backward.model = ols_step_backward_aic(backward_fit)
  prediction = predict(backward.model$model,test)
  squared_MSPE = mean((test$lSalePrice - prediction)^2)
  temp_RMSE = sqrt(squared_MSPE)
  total_RMSE = total_RMSE+temp_RMSE
}
#CV Press Estimate = 0.1481379
total_RMSE/iterations

#Build model with full dataset for fitting
backward_fit = lm(lSalePrice~.-Id,data=variables.used)
backward.model = ols_step_backward_aic(backward_fit)
backward.model$model
backward.model
ols_plot_resid_stand(backward.model$model)
ols_plot_cooksd_bar(backward.model$model)
plot(backward.model$model)

testVariable$lSalePriceBackward = predict(backward.model$model,testVariable)
testVariable$SalePrice = exp(testVariable$lSalePriceBackward)
result.backward = testVariable %>% dplyr::select(Id,SalePrice)
write.csv(result.backward,"Backward_Prediction_Miller_YU.csv",row.names = FALSE)


#Try Mass Library Forward model - Same result 0.15432
backward.model2 = stepAIC(backward_fit,direction="backward")
backward.model2

#Testing with Mass Library
testVariable$lSalePriceBackward = predict(backward.model2,testVariable)
testVariable$SalePrice = exp(testVariable$lSalePriceBackward)
result.backward = testVariable %>% dplyr::select(Id,SalePrice)
write.csv(result.backward,"Backward_Prediction_2_Miller_YU.csv",row.names = FALSE)
```

### Backward Selection - use p-value - Kaggle 0.15505
```{r}
set.seed(16)
#create forward fit model

#write a program to pretict 20 different combination to test forward selection model
iterations = 20
splitPerc = 0.95
total_RMSE = 0
for(i in 1:iterations){
  print(i)
  trainIndices = sample(1:dim(variables.used)[1],round(splitPerc * dim(variables.used)[1]))
  train = variables.used[trainIndices,]
  test = variables.used[-trainIndices,]
  backward_fit = lm(lSalePrice~.-Id,data=train)
  backward.model = ols_step_backward_p(backward_fit,prem=0.1,details=FALSE)
  prediction = predict(backward.model$model,test)
  squared_MSPE = mean((test$lSalePrice - prediction)^2)
  temp_RMSE = sqrt(squared_MSPE)
  total_RMSE = total_RMSE+temp_RMSE
}
#CV Press estimate = 0.1306067
total_RMSE/iterations
#Build model with full dataset for fitting
backward_fit = lm(lSalePrice~.-Id,data=variables.used)
backward.model = ols_step_backward_p(backward_fit,prem=0.1,details=FALSE)
backward.model$model
backward.model
ols_plot_resid_stand(backward.model$model)
ols_plot_cooksd_bar(backward.model$model)
plot(backward.model$model)

testVariable$lSalePriceBackward = predict(backward.model$model,testVariable)
testVariable$SalePrice = exp(testVariable$lSalePriceBackward)
result.backward = testVariable %>% dplyr::select(Id,SalePrice)
write.csv(result.backward,"Backward_Prediction_usingP_Miller_YU.csv",row.names = FALSE)
```


















